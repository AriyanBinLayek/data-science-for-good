{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import smac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ConfigSpace and different types of parameters\n",
    "from smac.configspace import ConfigurationSpace\n",
    "from ConfigSpace.hyperparameters import (CategoricalHyperparameter, \n",
    "                                         NormalFloatHyperparameter,\n",
    "                                            UniformFloatHyperparameter, \n",
    "            UniformIntegerHyperparameter, \n",
    "            IntegerHyperparameter)\n",
    "from ConfigSpace.conditions import InCondition\n",
    "\n",
    "# Import SMAC-utilities\n",
    "from smac.tae.execute_func import ExecuteTAFuncDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smac.scenario.scenario import Scenario\n",
    "from smac.facade.smac_facade import SMAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "space = {\n",
    "    'boosting_type': hp.choice('boosting_type', \n",
    "                              [{'boosting_type': 'gbdt', \n",
    "                                'subsample': hp.uniform('gdbt_subsample', 0.5, 1),\n",
    "                                'subsample_freq': hp.quniform('gbdt_subsample_freq', 1, 10, 1)}, \n",
    "                               {'boosting_type': 'dart', \n",
    "                                 'subsample': hp.uniform('dart_subsample', 0.5, 1),\n",
    "                                 'subsample_freq': hp.quniform('dart_subsample_freq', 1, 10, 1)},\n",
    "                                {'boosting_type': 'goss',\n",
    "                                 'subsample': 1.0,\n",
    "                                 'subsample_freq': 0}]),\n",
    "    'max_depth': hp.quniform('max_depth', -1, 50, 1)\n",
    "    'num_leaves': hp.quniform('num_leaves', 3, 50, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', \n",
    "                                   np.log(0.025), \n",
    "                                   np.log(0.25)),\n",
    "    'subsample_for_bin': hp.quniform('subsample_for_bin', 2000, 100000, 2000),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 5, 80, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.5, 1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(hyperparameters, nfolds=5):\n",
    "    \"\"\"Return validation score from hyperparameters for LightGBM\"\"\"\n",
    "    \n",
    "    # Keep track of evals\n",
    "    global ITERATION\n",
    "    ITERATION += 1\n",
    "    \n",
    "    # Retrieve the subsample\n",
    "    subsample = hyperparameters['boosting_type'].get('subsample', 1.0)\n",
    "    subsample_freq = hyperparameters['boosting_type'].get('subsample_freq', 0)\n",
    "    \n",
    "    # Extract the boosting type and subsample to top level keys\n",
    "    hyperparameters['boosting_type'] = hyperparameters['boosting_type']['boosting_type']\n",
    "    hyperparameters['subsample'] = subsample\n",
    "    hyperparameters['subsample_freq'] = subsample_freq\n",
    "    \n",
    "    # Make sure parameters that need to be integers are integers\n",
    "    for parameter_name in ['num_leaves', 'subsample_for_bin', \n",
    "                           'min_child_samples', 'subsample_freq']:\n",
    "        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n",
    "\n",
    "    if 'n_estimators' in hyperparameters:\n",
    "        del hyperparameters['n_estimators']\n",
    "    \n",
    "    # Using stratified kfold cross validation\n",
    "    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n",
    "    \n",
    "    # Convert to arrays for indexing\n",
    "    features = np.array(train_selected)\n",
    "    labels = np.array(train_labels).reshape((-1 ))\n",
    "    \n",
    "    valid_scores = []\n",
    "    best_estimators = []\n",
    "    run_times = []\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**hyperparameters, class_weight = 'balanced',\n",
    "                               n_jobs=-1, metric = 'None',\n",
    "                               n_estimators=10000)\n",
    "    \n",
    "    # Iterate through the folds\n",
    "    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n",
    "        \n",
    "        # Training and validation data\n",
    "        X_train = features[train_indices]\n",
    "        X_valid = features[valid_indices]\n",
    "        y_train = labels[train_indices]\n",
    "        y_valid = labels[valid_indices]\n",
    "        \n",
    "        start = timer()\n",
    "        # Train with early stopping\n",
    "        model.fit(X_train, y_train, early_stopping_rounds = 100, \n",
    "                  eval_metric = macro_f1_score, \n",
    "                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n",
    "                  eval_names = ['train', 'valid'],\n",
    "                  verbose = 400)\n",
    "        end = timer()\n",
    "        # Record the validation fold score\n",
    "        valid_scores.append(model.best_score_['valid']['macro_f1'])\n",
    "        best_estimators.append(model.best_iteration_)\n",
    "        \n",
    "        run_times.append(end - start)\n",
    "    \n",
    "    score = np.mean(valid_scores)\n",
    "    score_std = np.std(valid_scores)\n",
    "    loss = 1 - score\n",
    "    \n",
    "    run_time = np.mean(run_times)\n",
    "    run_time_std = np.std(run_times)\n",
    "    \n",
    "    estimators = int(np.mean(best_estimators))\n",
    "    hyperparameters['n_estimators'] = estimators\n",
    "    \n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(OUT_FILE, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, hyperparameters, ITERATION, run_time, score, score_std])\n",
    "    of_connection.close()\n",
    "    \n",
    "    # Display progress\n",
    "    if ITERATION % PROGRESS == 0:\n",
    "        display(f'Iteration: {ITERATION}, Current Score: {round(score, 4)}.')\n",
    "    \n",
    "#     return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,\n",
    "#             'time': run_time, 'time_std': run_time_std, 'status': STATUS_OK, \n",
    "#             'score': score, 'score_std': score_std}\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drop_rate | boosting_type in {'dart'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boosting_type = CategoricalHyperparameter('boosting_type',\n",
    "                                          ['goss', 'dart', 'gbdt'],\n",
    "                                          default_value = 'dart')\n",
    "\n",
    "max_depth = UniformIntegerHyperparameter('max_depth', -1, 50)\n",
    "\n",
    "num_leaves = UniformIntegerHyperparameter('num_leaves', 3, 50)\n",
    "\n",
    "\n",
    "learning_rate = NormalFloatHyperparameter('learning_rate', \n",
    "                                          0.025, 0.25)\n",
    "\n",
    "subsample_for_bin = UniformIntegerHyperparameter('subsample_for_bin', \n",
    "                                                 2000, 100000)\n",
    "\n",
    "min_child_samples = UniformIntegerHyperparameter('min_child_samples', 5, 80,\n",
    "                                                 default_value = 10)\n",
    "\n",
    "reg_alpha = UniformFloatHyperparameter('reg_alpha', 0.0, 1.0)\n",
    "reg_lambda = UniformFloatHyperparameter('reg_lambda', 0.0, 1.0)\n",
    "\n",
    "colsample_bytree = UniformFloatHyperparameter('colsample_bytree',\n",
    "                                              0.5, 1.0)\n",
    "\n",
    "\n",
    "subsample = UniformFloatHyperparameter('subsample', 0.5, 1.0,\n",
    "                                       default_value = 1.0)\n",
    "\n",
    "subsample_freq = UniformIntegerHyperparameter('subsample_freq', 1, 10,\n",
    "                                              default_value = None)\n",
    "\n",
    "drop_rate = UniformFloatHyperparameter('drop_rate', 0.1, 0.5)\n",
    "\n",
    "cs = ConfigurationSpace()\n",
    "\n",
    "cs.add_hyperparameters([boosting_type, max_depth, num_leaves, \n",
    "                        learning_rate, subsample_for_bin, min_child_samples,\n",
    "                        reg_alpha, reg_lambda, colsample_bytree, \n",
    "                        subsample, subsample_freq, drop_rate])\n",
    "\n",
    "cs.add_condition(InCondition(child=subsample, parent=boosting_type, \n",
    "                             values = ['gbdt', 'dart']))\n",
    "\n",
    "cs.add_condition(InCondition(child=subsample_freq, parent=boosting_type, \n",
    "                                            values = ['gbdt', 'dart']))\n",
    "\n",
    "cs.add_condition(InCondition(child=drop_rate, parent=boosting_type, \n",
    "                                            values = ['dart']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:smac.scenario.scenario.Scenario:Output to smac3-output_2018-08-14_21-05-31_549274\n"
     ]
    }
   ],
   "source": [
    "scenario = Scenario({\"run_obj\": \"quality\", \n",
    "                     \"runcount-limit\": 200,\n",
    "                     \"cs\": cs,\n",
    "                     \"deterministic\": \"true\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reg_alpha, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:smac.scenario.scenario.Scenario:Output to smac3-output_2018-08-14_20-31-22_984823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Value: 0.03\n"
     ]
    }
   ],
   "source": [
    "# We load the iris-dataset (a widely used benchmark)\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "def svm_from_cfg(cfg):\n",
    "    \"\"\" Creates a SVM based on a configuration and evaluates it on the\n",
    "    iris-dataset using cross-validation.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    cfg: Configuration (ConfigSpace.ConfigurationSpace.Configuration)\n",
    "        Configuration containing the parameters.\n",
    "        Configurations are indexable!\n",
    "    Returns:\n",
    "    --------\n",
    "    A crossvalidated mean score for the svm on the loaded data-set.\n",
    "    \"\"\"\n",
    "    # For deactivated parameters, the configuration stores None-values.\n",
    "    # This is not accepted by the SVM, so we remove them.\n",
    "    cfg = {k : cfg[k] for k in cfg if cfg[k]}\n",
    "    # We translate boolean values:\n",
    "    cfg[\"shrinking\"] = True if cfg[\"shrinking\"] == \"true\" else False\n",
    "    # And for gamma, we set it to a fixed value or to \"auto\" (if used)\n",
    "    if \"gamma\" in cfg:\n",
    "        cfg[\"gamma\"] = cfg[\"gamma_value\"] if cfg[\"gamma\"] == \"value\" else \"auto\"\n",
    "        cfg.pop(\"gamma_value\", None)  # Remove \"gamma_value\"\n",
    "\n",
    "    clf = svm.SVC(**cfg, random_state=42)\n",
    "\n",
    "    scores = cross_val_score(clf, iris.data, iris.target, cv=5)\n",
    "    return 1-np.mean(scores)  # Minimize!\n",
    "\n",
    "#logger = logging.getLogger(\"SVMExample\")\n",
    "logging.basicConfig(level=logging.INFO)  # logging.DEBUG for debug output\n",
    "\n",
    "# Build Configuration Space which defines all parameters and their ranges\n",
    "cs = ConfigurationSpace()\n",
    "\n",
    "# We define a few possible types of SVM-kernels and add them as \"kernel\" to our cs\n",
    "kernel = CategoricalHyperparameter(\"kernel\", [\"linear\", \"rbf\", \"poly\", \"sigmoid\"], default_value=\"poly\")\n",
    "cs.add_hyperparameter(kernel)\n",
    "\n",
    "# There are some hyperparameters shared by all kernels\n",
    "C = UniformFloatHyperparameter(\"C\", 0.001, 1000.0, default_value=1.0)\n",
    "shrinking = CategoricalHyperparameter(\"shrinking\", [\"true\", \"false\"], default_value=\"true\")\n",
    "cs.add_hyperparameters([C, shrinking])\n",
    "\n",
    "# Others are kernel-specific, so we can add conditions to limit the searchspace\n",
    "degree = UniformIntegerHyperparameter(\"degree\", 1, 5, default_value=3)     # Only used by kernel poly\n",
    "coef0 = UniformFloatHyperparameter(\"coef0\", 0.0, 10.0, default_value=0.0)  # poly, sigmoid\n",
    "cs.add_hyperparameters([degree, coef0])\n",
    "use_degree = InCondition(child=degree, parent=kernel, values=[\"poly\"])\n",
    "use_coef0 = InCondition(child=coef0, parent=kernel, values=[\"poly\", \"sigmoid\"])\n",
    "cs.add_conditions([use_degree, use_coef0])\n",
    "\n",
    "# This also works for parameters that are a mix of categorical and values from a range of numbers\n",
    "# For example, gamma can be either \"auto\" or a fixed float\n",
    "gamma = CategoricalHyperparameter(\"gamma\", [\"auto\", \"value\"], default_value=\"auto\")  # only rbf, poly, sigmoid\n",
    "gamma_value = UniformFloatHyperparameter(\"gamma_value\", 0.0001, 8, default_value=1)\n",
    "cs.add_hyperparameters([gamma, gamma_value])\n",
    "# We only activate gamma_value if gamma is set to \"value\"\n",
    "cs.add_condition(InCondition(child=gamma_value, parent=gamma, values=[\"value\"]))\n",
    "# And again we can restrict the use of gamma in general to the choice of the kernel\n",
    "cs.add_condition(InCondition(child=gamma, parent=kernel, values=[\"rbf\", \"poly\", \"sigmoid\"]))\n",
    "\n",
    "\n",
    "# Scenario object\n",
    "scenario = Scenario({\"run_obj\": \"quality\",   # we optimize quality (alternatively runtime)\n",
    "                     \"runcount-limit\": 200,  # maximum function evaluations\n",
    "                     \"cs\": cs,               # configuration space\n",
    "                     \"deterministic\": \"true\"\n",
    "                     })\n",
    "\n",
    "# Example call of the function\n",
    "# It returns: Status, Cost, Runtime, Additional Infos\n",
    "def_value = svm_from_cfg(cs.get_default_configuration())\n",
    "print(\"Default Value: %.2f\" % (def_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing! Depending on your machine, this might take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0333\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0333\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0333\n",
      "INFO:smac.intensification.intensification.Intensifier:Challenger (0.0267) is better than incumbent (0.0333) on 1 runs.\n",
      "INFO:smac.intensification.intensification.Intensifier:Changes in incumbent:\n",
      "INFO:smac.intensification.intensification.Intensifier:  C : 1.0 -> 10.939416021476587\n",
      "INFO:smac.intensification.intensification.Intensifier:  kernel : 'poly' -> 'linear'\n",
      "INFO:smac.intensification.intensification.Intensifier:  shrinking : 'true' -> 'false'\n",
      "INFO:smac.intensification.intensification.Intensifier:Challenger (0.0200) is better than incumbent (0.0267) on 1 runs.\n",
      "INFO:smac.intensification.intensification.Intensifier:Changes in incumbent:\n",
      "INFO:smac.intensification.intensification.Intensifier:  C : 10.939416021476587 -> 5.171268459600582\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0200\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0200\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0200\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0200\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0200\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0200\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0200\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0200\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0200\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0200\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0200\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0200\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0200\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0200\n",
      "INFO:smac.intensification.intensification.Intensifier:Challenger (0.0133) is better than incumbent (0.0200) on 1 runs.\n",
      "INFO:smac.intensification.intensification.Intensifier:Changes in incumbent:\n",
      "INFO:smac.intensification.intensification.Intensifier:  C : 5.171268459600582 -> 3.612669126119433\n",
      "INFO:smac.intensification.intensification.Intensifier:  gamma : None -> 'auto'\n",
      "INFO:smac.intensification.intensification.Intensifier:  kernel : 'linear' -> 'rbf'\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0133\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0133\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0133\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0133\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0133\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0133\n",
      "WARNING:smac.intensification.intensification.Intensifier:Challenger was the same as the current incumbent; Skipping challenger\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0133\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0133\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0133\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0133\n",
      "INFO:smac.intensification.intensification.Intensifier:Updated estimated cost of incumbent on 1 runs: 0.0133\n",
      "WARNING:smac.intensification.intensification.Intensifier:Challenger was the same as the current incumbent; Skipping challenger\n",
      "INFO:smac.stats.stats.Stats:##########################################################\n",
      "INFO:smac.stats.stats.Stats:Statistics:\n",
      "INFO:smac.stats.stats.Stats:#Incumbent changed: 3\n",
      "INFO:smac.stats.stats.Stats:#Target algorithm runs: 200 / 200.0\n",
      "INFO:smac.stats.stats.Stats:Used wallclock time: 11.64 / inf sec \n",
      "INFO:smac.stats.stats.Stats:Used target algorithm runtime: 7.62 / inf sec\n",
      "INFO:smac.stats.stats.Stats:##########################################################\n",
      "INFO:smac.facade.smac_facade.SMAC:Final Incumbent: Configuration:\n",
      "  C, Value: 3.612669126119433\n",
      "  gamma, Value: 'auto'\n",
      "  kernel, Value: 'rbf'\n",
      "  shrinking, Value: 'false'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Value: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Optimize, using a SMAC-object\n",
    "print(\"Optimizing! Depending on your machine, this might take a few minutes.\")\n",
    "smac = SMAC(scenario=scenario, rng=np.random.RandomState(42),\n",
    "        tae_runner=svm_from_cfg)\n",
    "\n",
    "incumbent = smac.optimize()\n",
    "\n",
    "inc_value = svm_from_cfg(incumbent)\n",
    "\n",
    "print(\"Optimized Value: %.2f\" % (inc_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "validate() got an unexpected keyword argument 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0153c2be0911>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m               \u001b[1;31m#instance_mode='train+test',  # Defines what instances to validate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m               \u001b[0mrepetitions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m        \u001b[1;31m# Ignored, unless you set \"deterministic\" to \"false\" in line 95\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m               n_jobs=1)               # How many cores to use in parallel for optimization\n\u001b[0m",
      "\u001b[1;32mc:\\users\\willk\\onedrive\\documents\\smac3\\smac\\facade\\smac_facade.py\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(self, config_mode, instance_mode, repetitions, use_epm, n_jobs, backend)\u001b[0m\n\u001b[0;32m    443\u001b[0m         \"\"\"\n\u001b[0;32m    444\u001b[0m         return self.solver.validate(config_mode, instance_mode, repetitions,\n\u001b[1;32m--> 445\u001b[1;33m                                     use_epm, n_jobs, backend)\n\u001b[0m\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_tae_runner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\willk\\onedrive\\documents\\smac3\\smac\\optimizer\\smbo.py\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(self, config_mode, instance_mode, repetitions, use_epm, n_jobs, backend)\u001b[0m\n\u001b[0;32m    297\u001b[0m                                         \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunhistory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintensifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtae_runner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m                                         output=new_rh_path)\n\u001b[0m\u001b[0;32m    300\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_rh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: validate() got an unexpected keyword argument 'output'"
     ]
    }
   ],
   "source": [
    "# We can also validate our results (though this makes a lot more sense with instances)\n",
    "smac.validate(config_mode='inc',      # We can choose which configurations to evaluate\n",
    "              #instance_mode='train+test',  # Defines what instances to validate\n",
    "              repetitions=100,        # Ignored, unless you set \"deterministic\" to \"false\" in line 95\n",
    "              n_jobs=1)               # How many cores to use in parallel for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

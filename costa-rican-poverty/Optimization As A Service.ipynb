{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization As A Service (OPTaaS)\n",
    "\n",
    "Testing out Mind Foundry's OPTaaS capabilities.\n",
    "These results will be compared to Bayesian Optimization using Hyperopt and SMAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Evaluation of the model\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willk\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (572) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10307, 2016)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.read_csv('data/ft_2000_important.csv')\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data \n",
    "\n",
    "Created by Featuretools automated feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_base = pd.read_csv('data/test.csv')[['Id', 'idhogar']]\n",
    "\n",
    "train = features[features['Target'].notnull()].copy()\n",
    "test = features[features['Target'].isnull()].copy()\n",
    "\n",
    "train_labels = np.array(train.pop('Target'))\n",
    "test_ids = list(test.pop('idhogar'))\n",
    "\n",
    "train, test = train.align(test, join = 'inner', axis = 1)\n",
    "\n",
    "for c in train:\n",
    "    if train[c].dtype == 'object':\n",
    "        train[c] = train[c].astype(np.float32)\n",
    "        test[c] = test[c].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train objects:  Index([], dtype='object')\n",
      "Test objects:  Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Train objects: ', train.columns[np.where(train.dtypes == 'object')])\n",
    "print('Test objects: ', test.columns[np.where(test.dtypes == 'object')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindfoundry.optaas.client.client import OPTaaSClient, Goal\n",
    "from mindfoundry.optaas.client.parameter import (Distribution, CategoricalParameter,\n",
    "                                                 IntParameter, ChoiceParameter, \n",
    "                                                 NumericParameter, FloatParameter)\n",
    "\n",
    "from mindfoundry.optaas.client.constraint import Constraint\n",
    "\n",
    "with open('C:/Users/willk/OneDrive/Desktop/optaas_key.txt', 'r') as f:\n",
    "    api_key = str(f.read())\n",
    "    \n",
    "client = OPTaaSClient('https://optaas.mindfoundry.ai', api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Function\n",
    "\n",
    "Takes in hyperparameters and returns a score to maximize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1_score(labels, predictions):\n",
    "    # Reshape the predictions as needed\n",
    "    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n",
    "    \n",
    "    metric_value = f1_score(labels, predictions, average = 'macro')\n",
    "    \n",
    "    # Return is name, value, is_higher_better\n",
    "    return 'macro_f1', metric_value, True\n",
    "\n",
    "def objective(num_leaves, learning_rate, boosting_type,\n",
    "                      subsample, subsample_for_bin, min_child_samples,\n",
    "                      reg_alpha, reg_lambda, colsample_bytree, nfolds=5):\n",
    "    \"\"\"Return validation score from hyperparameters for LightGBM\"\"\"\n",
    "\n",
    "    # Using stratified kfold cross validation\n",
    "    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n",
    "    \n",
    "    # Convert to arrays for indexing\n",
    "    features = np.array(train)\n",
    "    labels = np.array(train_labels).reshape((-1 ))\n",
    "    \n",
    "    valid_scores = []\n",
    "    best_estimators = []\n",
    "    \n",
    "    model = lgb.LGBMClassifier(num_leaves=num_leaves, learning_rate=learning_rate,\n",
    "                               boosting_type=boosting_type, subsample=subsample,\n",
    "                               subsample_for_bin=subsample_for_bin, \n",
    "                               min_child_samples=min_child_samples,\n",
    "                               reg_alpha=reg_alpha, reg_lambda=reg_lambda, \n",
    "                               colsample_bytree=colsample_bytree,\n",
    "                               class_weight = 'balanced',\n",
    "                               n_jobs=-1, n_estimators=10000)\n",
    "    \n",
    "    # Iterate through the folds\n",
    "    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n",
    "        \n",
    "        # Training and validation data\n",
    "        X_train = features[train_indices]\n",
    "        X_valid = features[valid_indices]\n",
    "        y_train = labels[train_indices]\n",
    "        y_valid = labels[valid_indices]\n",
    "        \n",
    "        # Train with early stopping\n",
    "        model.fit(X_train, y_train, early_stopping_rounds = 100, \n",
    "                  eval_metric = macro_f1_score,\n",
    "                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n",
    "                  eval_names = ['train', 'valid'],\n",
    "                  verbose = -1)\n",
    "        \n",
    "        # Record the validation fold score\n",
    "        valid_scores.append(model.best_score_['valid']['macro_f1'])\n",
    "        best_estimators.append(model.best_iteration_)\n",
    "        \n",
    "    best_estimators = np.array(best_estimators)\n",
    "    valid_scores = np.array(valid_scores)\n",
    "    \n",
    "#     return valid_scores, best_estimators\n",
    "\n",
    "    # Write to the csv file ('a' means append)\n",
    "#     of_connection = open(OUT_FILE, 'a')\n",
    "#     writer = csv.writer(of_connection)\n",
    "#     writer.writerow([loss, hyperparameters, ITERATION, run_time, best_score, best_std])\n",
    "#     of_connection.close()\n",
    "\n",
    "    # Dictionary with information for evaluation\n",
    "#     return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,\n",
    "#             'train_time': run_time, 'status': STATUS_OK}\n",
    "\n",
    "    return valid_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Define the hyperparameter distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosting_type = CategoricalParameter('boosting_type', \n",
    "                                     values = ['gbdt', 'dart', 'goss'], \n",
    "                                     id='boosting_type')\n",
    "\n",
    "num_leaves = IntParameter('num_leaves', minimum=3, \n",
    "                          maximum=50, id='num_leaves')\n",
    "\n",
    "learning_rate = FloatParameter('learning_rate', minimum=0.025, \n",
    "                               maximum=0.25, id='learning_rate',\n",
    "                               distribution=Distribution.LOGUNIFORM)\n",
    "\n",
    "subsample = FloatParameter('subsample', minimum=0.5, \n",
    "                           maximum=1.0, id='subsample')\n",
    "\n",
    "subsample_for_bin = IntParameter('subsample_for_bin', minimum=2000, \n",
    "                                 maximum=100000, id='subsample_for_bin')\n",
    "\n",
    "min_child_samples = IntParameter('min_child_samples', minimum=5, \n",
    "                                 maximum=80, id='min_child_samples')\n",
    "\n",
    "reg_alpha = FloatParameter('reg_alpha', minimum=0.0, \n",
    "                           maximum=1.0, id='reg_alpha')\n",
    "\n",
    "reg_lambda = FloatParameter('reg_lambda', minimum=0.0, \n",
    "                            maximum=1.0, id='reg_lambda')\n",
    "\n",
    "colsample_bytree = FloatParameter('colsample_bytree', minimum=0.5, \n",
    "                                  maximum=1.0, id='colsample_bytree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_constraint = Constraint(when=boosting_type=='goss', \n",
    "                                  then=subsample==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = client.create_task(\n",
    "        title = 'Light GBM Opt',\n",
    "        goal = Goal.max,\n",
    "        parameters = [num_leaves, learning_rate, boosting_type,\n",
    "                      subsample, subsample_for_bin, min_child_samples,\n",
    "                      reg_alpha, reg_lambda, colsample_bytree],\n",
    "         constraints = [ Constraint(when=boosting_type=='goss', \n",
    "                                    then=subsample==1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "best_result, best_configuration = task.run(objective, max_iterations = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ 'id': '132dc8d6-bab1-4695-b058-6fcbdbf21684',\n",
       "  'type': 'exploitation',\n",
       "  'values': { 'boosting_type': 'dart',\n",
       "              'colsample_bytree': 0.9843467236959204,\n",
       "              'learning_rate': 0.11598629586769524,\n",
       "              'min_child_samples': 44,\n",
       "              'num_leaves': 49,\n",
       "              'reg_alpha': 0.35397370408131534,\n",
       "              'reg_lambda': 0.5904910774606467,\n",
       "              'subsample': 0.6299872254632797,\n",
       "              'subsample_for_bin': 60611}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ 'configuration': '132dc8d6-bab1-4695-b058-6fcbdbf21684',\n",
       "  'id': 3250,\n",
       "  'score': 0.4629755551376399,\n",
       "  'user_defined_data': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dump() missing 1 required positional argument: 'fp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6778356c39bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'task_results.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: dump() missing 1 required positional argument: 'fp'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('task_results.txt', 'w') as f:\n",
    "    json.dump(str(task.get_results()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
